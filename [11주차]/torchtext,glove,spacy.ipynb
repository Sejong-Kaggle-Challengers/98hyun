{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndel train_data['id']\n\ntrain_data['keyword'].fillna('none',inplace=True)\ntrain_data['location'].fillna('none',inplace=True)\n\ntest_data=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_data['keyword'].fillna('none',inplace=True)\ntest_data['location'].fillna('none',inplace=True)\n\ntrain_data=train_data.sample(frac=1)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def refine_text(text):\n    text=re.sub(r'[^a-z\\s]',r'',re.sub(r'http\\S+',r'',re.sub(r'#','',text).lower()))\n    return text\n\nfor col in ['keyword','location','text']:\n    train_data[col]=train_data[col].apply(lambda x:refine_text(x))\n    test_data[col]=test_data[col].apply(lambda x:refine_text(x))\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data,val_data=train_test_split(train_data,test_size=0.1,shuffle=True,random_state=71)\nlen(train_data),len(val_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nspacy_nlp=spacy.load('en')\n\nfrom nltk.corpus import stopwords\nstopword_list=stopwords.words('english')\n\ndef tokenizer(text, MAX_LEN=20000):\n    text=re.sub(' +',' ',re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\",\" \",text))\n    text=text if len(text)<=MAX_LEN else text[:MAX_LEN]\n    return [x.text for x in spacy_nlp.tokenizer(text) if (x.text!=' ') and (x.text not in stopword_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://nlp.stanford.edu/projects/glove/\n\nfrom torchtext.vocab import Vectors,Vocab\nfrom collections import Counter\n\ngloveVectors=Vectors(name='/kaggle/input/glove6b/glove.6B.100d.txt')\n\nfor i in train_data.index:\n    counter.update(tokenizer(train_data['text'][i]+' '+train_data['keyword'][i]+' '+train_data['location'][i]))\n    \nvocabulary=Vocab(counter,max_size=20000,min_freq=2,vectors=gloveVectors,specials=['<pad>','<unk>'])\n\nprint('Embedding vocab size: ', vocabulary.vectors.size(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchtext \n\nclass ClassifyDataset(torchtext.data.Dataset):\n    def __init__(self, df, fields, train=True, **kwargs):\n        examples=[]\n        for i, row in df.iterrows():\n            examples.append(torchtext.data.Example.fromlist([row.text, row.target if train else None],fields))\n        super().__init__(examples, fields, **kwargs)\n        \n    @staticmethod\n    def sort_key(x):\n        return len(x.text)\n    \n    @classmethod\n    def splits(cls, fields, train_df=None, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data=(None, None, None)\n        \n        if train_df is not None:\n            train_data=cls(train_df.copy(), fields, **kwargs)\n            \n        if val_df is not None:\n            val_data=cls(val_df.copy(), fields, **kwargs)\n            \n        if test_df is not None:\n            test_data=cls(test_df.copy(), fields, train=False, **kwargs)\n            \n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nText=torchtext.data.Field(tokenizer, include_lengths=True)\nLabel=torchtext.data.LabelField(dtype=torch.float)\n\nfields= [('text', Text),('label', Label)]\n\ntrain_ds, val_ds, test_ds= ClassifyDataset.splits(fields, train_df=train_data, val_df=val_data, test_df=test_data)\n\n#sampling random example\nprint(vars(train_ds[61]), vars(val_ds[61]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Text.build_vocab(train_ds,\n                max_size=20000,\n                vectors=gloveVectors,\n                unk_init=torch.Tensor.zero_)\n\nLabel.build_vocab(train_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\ndevice= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator= torchtext.data.BucketIterator.splits(\n                                (train_ds, val_ds),\n                                batch_size=batch_size,\n                                sort_within_batch=True,\n                                device=device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model - Simple LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs=20\nlr=0.001\n\ninput_dims=len(Text.vocab)\nembedding_dims=100\nhidden_dims=256\noutput_dims=1\nn_layers=2\nbidirectional=True\ndrop=0.2\n\npad_idx=Text.vocab.stoi[Text.pad_token]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(preds, y):\n    rounded_preds=torch.round(torch.sigmoid(preds))\n    correct=(rounded_preds==y).float()\n    return correct.sum()/len(correct)\n\nclass LSTMnn(torch.nn.Module):\n    def __init__(self, vocab_size, embedding_dims, hidden_dims, output_dims, n_layers, bidirectional,pad_idx, dropout):\n        super().__init__()\n        self.embeddings=torch.nn.Embedding(vocab_size, embedding_dims, padding_idx=pad_idx)\n        self.lstm=torch.nn.LSTM(embedding_dims, hidden_dims,\n                               num_layers=n_layers,\n                               bidirectional=bidirectional,\n                               dropout=dropout)\n        self.fc1=torch.nn.Linear(hidden_dims*2, hidden_dims)\n        self.fc2=torch.nn.Linear(hidden_dims, output_dims)\n        self.dropout=torch.nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths, train=True):\n        \n        #text and text_lengths : [seq_len, batch_size] and [batch_size]\n        embedding=self.embeddings(text) #[seq_len, batch_size, emb_size]\n        packed_embeddings=torch.nn.utils.rnn.pack_padded_sequence(embedding, text_lengths)\n        packed_out,(hidden,cell)=self.lstm(packed_embeddings)\n        #hidden:[num_layers*num_dir, batch size, hidden dims]\n        hidden=self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        \n        output=self.fc1(hidden)\n        output=self.dropout(self.fc2(output))\n        \n        return output\n        \nmodel=LSTMnn(vocab_size=input_dims,embedding_dims=embedding_dims, \n             hidden_dims=hidden_dims,output_dims=output_dims,\n             n_layers=n_layers, bidirectional=bidirectional,\n             pad_idx=pad_idx, dropout=drop)\n\nmodel.embeddings.weight.data.copy_(Text.vocab.vectors)\n\nmodel.to(device)\n\ncriterion=torch.nn.BCEWithLogitsLoss()\noptimizer=torch.optim.Adam(model.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in train_iterator:\n#     model(i.text[0], i.text[1], train=True)\n#     break\n# torch.Size([1, 4]) tensor([4]) torch.Size([1])\n# torch.Size([1, 4, 100]) torch.Size([1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator):\n    \n    epoch_loss=0\n    epoch_acc=0\n    \n    model.train()\n    \n    for batch in iterator:\n        text, text_len=batch.text\n        \n        optimizer.zero_grad()\n        pred=model(text, text_len).squeeze(1)\n        \n        loss=criterion(pred, batch.label)\n        acc=accuracy(pred, batch.label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss+=loss.item()\n        epoch_acc+=acc.item()\n    \n    return (epoch_loss/len(iterator), epoch_acc/len(iterator))\n\ndef evaluate(model, iterator):\n\n    epoch_acc=0\n    \n    model.eval()\n\n    with torch.no_grad():\n        for batch in iterator:\n            text, text_len=batch.text\n\n            pred=model(text, text_len).squeeze(1)\n\n            acc=accuracy(pred, batch.label)\n            \n            epoch_acc+=acc.item()\n    \n    return epoch_acc/len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nt = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    train_loss, train_acc = train(model, train_iterator)\n    valid_acc = evaluate(model, valid_iterator)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.save(model.state_dict(),'model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(range(len(loss)), loss,color='red',label='loss')\nplt.plot(range(len(acc)), acc,color='blue',label='acc')\nplt.plot(range(len(val_acc)), val_acc,color='green',label='val_acc')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def infer(text):\n    text_arr=[]\n    for i in tokenizer(text):\n        text_arr.append(Text.vocab.stoi[i])\n        \n    if len(text_arr)>0:\n        model.eval()\n        with torch.no_grad():\n#           gpu\n#           text=torch.LongTensor([text_arr]).view(-1,1).to(device)\n#           text_len=torch.LongTensor([text.shape[1]]).to(device)\n            # cpu\n            text=torch.LongTensor([text_arr]).view(-1,1)\n            text_len=torch.LongTensor([text.shape[1]])\n            return int(torch.round(torch.sigmoid(model(text, text_len).squeeze(1))).item())\n    else:\n        return 0\n\ntest_preds=[]\nfor i in test_data.iterrows():\n    test_preds.append(infer(i[1]['text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submissions=pd.DataFrame({'id':test_data['id'].values,'target':test_preds})\nmy_submissions.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport csv\n\nimport fasttext","execution_count":146,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmit=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":147,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.isna().sum()\n\ntrain['keyword']=train['keyword'].fillna('none')\ntrain['location']=train['location'].fillna('none')\ntest['keyword']=test['keyword'].fillna('none')\ntest['location']=test['location'].fillna('none')","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nTrain=train.drop('target',axis=1)\nTarget=train['target']\n\nX_tr,X_val,y_tr,y_val=train_test_split(Train,Target,test_size=0.15,random_state=71,stratify=train['target'])","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_arr=[]\nval_arr=[]\ntest_arr=[]\n\nfor i,row in X_tr.iterrows():\n    target=y_tr.loc[i]\n    label=f'__label__{target}'\n    text=row['keyword']+' '+row['location']+' '+row['text']\n    label+=' '+text\n    tr_arr.append(label)\n    \nfor i,row in X_val.iterrows():\n    text=row['keyword']+' '+row['location']+' '+row['text']\n    val_arr.append(text)\n    \nfor i,row in test.iterrows():\n    text=row['keyword']+' '+row['location']+' '+row['text']\n    test_arr.append(text)","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.DataFrame(train_np)\ntrain_df.to_csv('train.txt',index=False,sep=' ',header=False,quoting=csv.QUOTE_NONE,quotechar=\"\",escapechar=\" \")","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=fasttext.train_supervised('train.txt',label_prefix='__label__',epoch=10)\nprint(model.labels,'are the labels or targets the model is predicting')","execution_count":152,"outputs":[{"output_type":"stream","text":"['__label__0', '__label__1'] are the labels or targets the model is predicting\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nval_arr=[re.sub(r'\\n','',text) for text in val_arr]\n\npred=[int(label[0][-1]) for label in model.predict(val_arr)[0]]\nprint(f'val_acc : {accuracy_score(pred,y_val.values)}')","execution_count":153,"outputs":[{"output_type":"stream","text":"acc : 0.9807355516637478\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_arr=[re.sub(r'\\n','',text) for text in test_arr]\n\npred=[int(label[0][-1]) for label in model.predict(test_arr)[0]]","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['target']=pred\nsubmit.head()","execution_count":155,"outputs":[{"output_type":"execute_result","execution_count":155,"data":{"text/plain":"   id  target\n0   0       1\n1   2       0\n2   3       1\n3   9       0\n4  11       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv',index=False)","execution_count":156,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}