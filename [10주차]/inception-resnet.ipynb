{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# inception"},{"metadata":{},"cell_type":"markdown","source":"![](https://norman3.github.io/papers/images/google_inception/f01.png)"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2953/1*ZFPOSAted10TPd3hBQU8iQ.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchinfo","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting torchinfo\n  Downloading torchinfo-0.0.8-py3-none-any.whl (16 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-0.0.8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchinfo import summary","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# b 모형 구현.\n\n# 각각의 모형\ndef conv_1(in_channels,out_channels):\n    model=nn.Sequential(\n    nn.Conv2d(in_channels,out_channels,kernel_size=1,padding=1,stride=1),\n    nn.ReLU())\n    return model\n\ndef conv_1_3(in_channels,mid_channels,out_channels):\n    model=nn.Sequential(\n    nn.Conv2d(in_channels,mid_channels,kernel_size=1,padding=1,stride=1),\n    nn.ReLU(),\n    nn.Conv2d(mid_channels,out_channels,kernel_size=3,padding=1,stride=1),\n    nn.ReLU())\n    return model\n\ndef conv_1_5(in_channels,mid_channels,out_channels):\n    model=nn.Sequential(\n    nn.Conv2d(in_channels,mid_channels,kernel_size=1,padding=1,stride=1),\n    nn.ReLU(),\n    nn.Conv2d(mid_channels,out_channels,kernel_size=5,padding=2,stride=1),\n    nn.ReLU())\n    return model\n\ndef max_3_1(in_channels,out_channels):\n    model=nn.Sequential(\n    nn.MaxPool2d(kernel_size=3,padding=1,stride=1),\n    nn.Conv2d(in_channels,out_channels,kernel_size=1,padding=1,stride=1),\n    nn.ReLU())\n    return model\n\n\n# inception -> 이것 또한 구성요소\nclass inception(nn.Module):\n    def __init__(self,in_channels,out_channels_1,mid_channels_3,out_channels_3,mid_channels_5,out_channels_5,pool):\n        super(inception,self).__init__()\n        \n        self.conv_1=conv_1(in_channels,out_channels_1)\n        self.conv_1_3=conv_1_3(in_channels,mid_channels_3,out_channels_3)\n        self.conv_1_5=conv_1_5(in_channels,mid_channels_5,out_channels_5)\n        self.max_3_1=max_3_1(in_channels,pool)\n    \n    def forward(self,x):\n        out_1=self.conv_1(x)\n        out_2=self.conv_1_3(x)\n        out_3=self.conv_1_5(x)\n        out_4=self.max_3_1(x)\n        output=torch.cat([out_1,out_2,out_3,out_4],dim=1) # N,C,H,W 의 C (0,1,2,3) 의 1\n        return output\n    \n# 최종 GoogLeNet 같은 inception을 활용한 모델.\nclass GoogLeNet(nn.Module):\n    def __init__(self,base_dim,num_classes=1000):\n        super(GoogLeNet,self).__init__()\n        \n        self.layer_1=nn.Sequential(\n        nn.Conv2d(3,base_dim,7,2,3), # in_channels,out_channels,kernel_size,stride,padding 순.\n        nn.MaxPool2d(3,2,1), # kernel_size,stride,padding 순.\n        nn.Conv2d(base_dim,base_dim*3,3,1,1), # \n        nn.MaxPool2d(3,2,1))\n        \n        self.layer_2=nn.Sequential(\n        # in_channels,out_channels_1,mid_channels_3,out_channels_3,mid_channels_5,out_channels_5,pool\n        inception(base_dim*3,64,96,128,16,32,32),\n        inception(256,128,128,192,32,96,64),\n        nn.MaxPool2d(3,2,1)\n        )\n        \n        self.layer_3=nn.Sequential(\n        inception(480,192,96,208,16,48,64),\n        inception(512,160,112,224,24,64,64), # 알아보기 어렵다면, 0,1,2.. 라고 할때\n        inception(512,128,128,256,24,64,64), # 1,3,5,6 만 더한게 in_channels로 간다.\n        inception(512,112,144,288,32,64,64), # 바로 왼쪽을 예로 112+288+64+64=528\n        inception(528,256,160,320,32,128,128),# 여기 528에서 256+320+128+128=832\n        nn.MaxPool2d(3,2,1)\n        )\n        \n        self.layer_4=nn.Sequential(\n        inception(832,256,160,320,32,128,128),\n        inception(832,384,192,384,48,128,128),\n        nn.AvgPool2d(7,1) # kernel_size,stride 순.\n        )\n        \n        self.layer_5=nn.Dropout2d(0.4) # p 확률\n        self.fc_layer=nn.Linear(1024,num_classes)\n        \n    def forward(self,x):\n        out=self.layer_1(x)\n        out=self.layer_2(out)\n        out=self.layer_3(out)\n        out=self.layer_4(out)\n        out=self.layer_5(out)\n        # 원래 이게 맞는데\n#       out=out.view(out.size(0),-1)\n#       out=out.view(batch_size,-1) 이렇게 쓰여있었거든요. 근데 입출력 feature는 맞춰야하니까\n# 아마 입력 H,W에서 틀린것같아요 마지막에 1,1024,1,1 이렇게 나와야 하는데 마지막에 1,1024,11,11 이래서 뭔가 이상한것같습니다.\n        out=out.view(-1,1024)\n        out=self.fc_layer(out)\n        return out","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GoogLeNet(3)\nsummary(model,(1,3,224,224))","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [1, 9, 28, 28]            --\n|    └─Conv2d: 2-1                       [1, 3, 112, 112]          444\n|    └─MaxPool2d: 2-2                    [1, 3, 56, 56]            --\n|    └─Conv2d: 2-3                       [1, 9, 56, 56]            252\n|    └─MaxPool2d: 2-4                    [1, 9, 28, 28]            --\n├─Sequential: 1-2                        [1, 480, 16, 16]          --\n|    └─inception: 2-5                    [1, 256, 30, 30]          --\n|    |    └─Sequential: 3-1              [1, 64, 30, 30]           640\n|    |    └─Sequential: 3-2              [1, 128, 30, 30]          111,680\n|    |    └─Sequential: 3-3              [1, 32, 30, 30]           12,992\n|    |    └─Sequential: 3-4              [1, 32, 30, 30]           320\n|    └─inception: 2-6                    [1, 480, 32, 32]          --\n|    |    └─Sequential: 3-5              [1, 128, 32, 32]          32,896\n|    |    └─Sequential: 3-6              [1, 192, 32, 32]          254,272\n|    |    └─Sequential: 3-7              [1, 96, 32, 32]           85,120\n|    |    └─Sequential: 3-8              [1, 64, 32, 32]           16,448\n|    └─MaxPool2d: 2-7                    [1, 480, 16, 16]          --\n├─Sequential: 1-3                        [1, 832, 13, 13]          --\n|    └─inception: 2-8                    [1, 512, 18, 18]          --\n|    |    └─Sequential: 3-9              [1, 192, 18, 18]          92,352\n|    |    └─Sequential: 3-10             [1, 208, 18, 18]          226,096\n|    |    └─Sequential: 3-11             [1, 48, 18, 18]           26,944\n|    |    └─Sequential: 3-12             [1, 64, 18, 18]           30,784\n|    └─inception: 2-9                    [1, 512, 20, 20]          --\n|    |    └─Sequential: 3-13             [1, 160, 20, 20]          82,080\n|    |    └─Sequential: 3-14             [1, 224, 20, 20]          283,472\n|    |    └─Sequential: 3-15             [1, 64, 20, 20]           50,776\n|    |    └─Sequential: 3-16             [1, 64, 20, 20]           32,832\n|    └─inception: 2-10                   [1, 512, 22, 22]          --\n|    |    └─Sequential: 3-17             [1, 128, 22, 22]          65,664\n|    |    └─Sequential: 3-18             [1, 256, 22, 22]          360,832\n|    |    └─Sequential: 3-19             [1, 64, 22, 22]           50,776\n|    |    └─Sequential: 3-20             [1, 64, 22, 22]           32,832\n|    └─inception: 2-11                   [1, 528, 24, 24]          --\n|    |    └─Sequential: 3-21             [1, 112, 24, 24]          57,456\n|    |    └─Sequential: 3-22             [1, 288, 24, 24]          447,408\n|    |    └─Sequential: 3-23             [1, 64, 24, 24]           67,680\n|    |    └─Sequential: 3-24             [1, 64, 24, 24]           32,832\n|    └─inception: 2-12                   [1, 832, 26, 26]          --\n|    |    └─Sequential: 3-25             [1, 256, 26, 26]          135,424\n|    |    └─Sequential: 3-26             [1, 320, 26, 26]          545,760\n|    |    └─Sequential: 3-27             [1, 128, 26, 26]          119,456\n|    |    └─Sequential: 3-28             [1, 128, 26, 26]          67,712\n|    └─MaxPool2d: 2-13                   [1, 832, 13, 13]          --\n├─Sequential: 1-4                        [1, 1024, 11, 11]         --\n|    └─inception: 2-14                   [1, 832, 15, 15]          --\n|    |    └─Sequential: 3-29             [1, 256, 15, 15]          213,248\n|    |    └─Sequential: 3-30             [1, 320, 15, 15]          594,400\n|    |    └─Sequential: 3-31             [1, 128, 15, 15]          129,184\n|    |    └─Sequential: 3-32             [1, 128, 15, 15]          106,624\n|    └─inception: 2-15                   [1, 1024, 17, 17]         --\n|    |    └─Sequential: 3-33             [1, 384, 17, 17]          319,872\n|    |    └─Sequential: 3-34             [1, 384, 17, 17]          823,872\n|    |    └─Sequential: 3-35             [1, 128, 17, 17]          193,712\n|    |    └─Sequential: 3-36             [1, 128, 17, 17]          106,624\n|    └─AvgPool2d: 2-16                   [1, 1024, 11, 11]         --\n├─Dropout2d: 1-5                         [1, 1024, 11, 11]         --\n├─Linear: 1-6                            [121, 1000]               1,025,000\n==========================================================================================\nTotal params: 6,836,768\nTrainable params: 6,836,768\nNon-trainable params: 0\nTotal mult-adds (G): 2.67\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 29.20\nParams size (MB): 27.35\nEstimated Total Size (MB): 57.15\n=========================================================================================="},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# resnet"},{"metadata":{},"cell_type":"markdown","source":"![](https://neurohive.io/wp-content/uploads/2019/01/resnet-e1548261477164.png)"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.stack.imgur.com/kbiIG.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bottleneck 블록을 위한. inception 같은거라고 보면 될거같다.\n\ndef conv_block_1(in_channels,out_channels,stride=1):\n    model=nn.Sequential(\n    nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,padding=1),\n    nn.ReLU())\n    return model\n\ndef conv_block_3(in_channels,out_channels):\n    model=nn.Sequential(\n    nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1),\n    nn.ReLU())\n    return model\n\nclass BottleNeck(nn.Module):\n    def __init__(self,in_channels,mid_channels,out_channels,down=False):\n        super(BottleNeck,self).__init__()\n        self.down=down\n        \n        if self.down:\n            self.layer=nn.Sequential(\n            conv_block_1(in_channels,mid_channels,stride=2),\n            conv_block_3(mid_channels,mid_channels),\n            conv_block_1(mid_channels,out_channels))\n        \n            self.downsample=nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=2)\n        else:\n            self.layer=nn.Sequential(\n            conv_block_1(in_channels,mid_channels),\n            conv_block_3(mid_channels,mid_channels),\n            conv_block_1(mid_channels,out_channels))\n            \n            self.dim_equalizer=nn.Conv2d(in_channels,out_channels,kernel_size=1)\n            \n    def forward(self,x):\n        if self.down:\n            downsample=self.downsample(x)\n            out=self.layer(x)\n            out=out+downsample\n        \n        else:\n            out=self.layer(x)\n            if x.size() is not out.size():\n                x=self.dim_equalizer(x)\n            \n            out=out+x\n        return out","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self,base_dim,num_classes=1000):\n        super(ResNet,self).__init__()\n        \n        self.layer_1=nn.Sequential(\n        nn.Conv2d(3,base_dim,7,2,3),\n        nn.ReLU(),\n        nn.MaxPool2d(3,2,1))\n        \n        self.layer_2=nn.Sequential(\n        BottleNeck(base_dim,base_dim,base_dim*4),\n        BottleNeck(base_dim*4,base_dim,base_dim*4),\n        BottleNeck(base_dim*4,base_dim,base_dim*4,down=True))\n        \n        self.layer_3=nn.Sequential(\n        BottleNeck(base_dim*4,base_dim*2,base_dim*8),\n        BottleNeck(base_dim*8,base_dim*2,base_dim*8),\n        BottleNeck(base_dim*8,base_dim*2,base_dim*8),\n        BottleNeck(base_dim*8,base_dim*2,base_dim*8,down=True))\n        \n        self.layer_4=nn.Sequential(\n        BottleNeck(base_dim*8,base_dim*4,base_dim*16),\n        BottleNeck(base_dim*16,base_dim*4,base_dim*16),\n        BottleNeck(base_dim*16,base_dim*4,base_dim*16),\n        BottleNeck(base_dim*16,base_dim*4,base_dim*16),\n        BottleNeck(base_dim*16,base_dim*4,base_dim*16),\n        BottleNeck(base_dim*16,base_dim*4,base_dim*16,down=True))\n        \n        self.layer_5=nn.Sequential(\n        BottleNeck(base_dim*16,base_dim*8,base_dim*32),\n        BottleNeck(base_dim*32,base_dim*8,base_dim*32),\n        BottleNeck(base_dim*32,base_dim*8,base_dim*32))\n        \n        self.avgpool=nn.AvgPool2d(7,1)\n        self.fc_layer=nn.Linear(base_dim*32,num_classes)\n        \n    def forward(self,x):\n        out=self.layer_1(x)\n        out=self.layer_2(out)\n        out=self.layer_3(out)\n        out=self.layer_4(out)\n        out=self.layer_5(out)\n        out=self.avgpool(out)\n        out=out.view(out.size(0),-1)\n        out=self.fc_layer(out)\n        return out","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이건 image에 보통 많이 사용하나봐요 그래서 잔차학습을 했던 저번 반도체박막두께 대회 공부했던거하고\n# 저희 main repo 의 issue를 보고 했는데 이건 x. 일단 뭔가 잘못된거같은데 모르겠습니다.\nmodel=ResNet(3)\nsummary(model,(1,3,224,224))","execution_count":9,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1-1, Conv2d: 2-1, ReLU: 2-2, MaxPool2d: 2-3, Sequential: 3-1, Sequential: 4-1, Sequential: 4-2, Sequential: 4-3, Conv2d: 3-2]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, col_names, col_width, depth, device, dtypes, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-8-10419a4bb232>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n","\u001b[0;32m<ipython-input-7-65c8c0e8a1e6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (60) must match the size of tensor b (56) at non-singleton dimension 3","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-0da91add5e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 저희 main repo 의 issue를 보고\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, col_names, col_width, depth, device, dtypes, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             ) from e\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Sequential: 1-1, Conv2d: 2-1, ReLU: 2-2, MaxPool2d: 2-3, Sequential: 3-1, Sequential: 4-1, Sequential: 4-2, Sequential: 4-3, Conv2d: 3-2]"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)","execution_count":10,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907689d1a13044a896ef27bb3eaa2afe"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model=ResNet(3)\nsummary(model,(1,3,224,224))","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n├─ReLU: 1-3                              [1, 64, 112, 112]         --\n├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n├─Sequential: 1-5                        [1, 64, 56, 56]           --\n|    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n|    |    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n|    |    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n|    |    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n|    |    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n|    |    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n|    |    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n|    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n|    |    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n|    |    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n|    |    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n|    |    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n|    |    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n|    |    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n├─Sequential: 1-6                        [1, 128, 28, 28]          --\n|    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n|    |    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n|    |    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n|    |    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n|    |    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n|    |    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n|    |    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n|    |    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n|    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n|    |    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n|    |    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n|    |    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n|    |    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n|    |    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n|    |    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n├─Sequential: 1-7                        [1, 256, 14, 14]          --\n|    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n|    |    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n|    |    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n|    |    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n|    |    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n|    |    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n|    |    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n|    |    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n|    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n|    |    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n|    |    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n|    |    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n|    |    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n|    |    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n|    |    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n├─Sequential: 1-8                        [1, 512, 7, 7]            --\n|    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n|    |    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n|    |    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n|    |    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n|    |    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n|    |    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n|    |    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n|    |    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n|    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n|    |    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n|    |    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n|    |    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n|    |    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n|    |    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n|    |    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n├─Linear: 1-10                           [1, 1000]                 513,000\n==========================================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\nTotal mult-adds (G): 1.84\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 39.75\nParams size (MB): 46.76\nEstimated Total Size (MB): 87.11\n=========================================================================================="},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 저번 반도체 박막 두께 1등 resnet"},{"metadata":{},"cell_type":"markdown","source":"![](https://ifh.cc/g/KdpCm5.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet, self).__init__()\n        \n        self.ln = nn.LayerNorm(10000)\n        self.ln1 = nn.LayerNorm(7000)\n        self.ln2 = nn.LayerNorm(4000)\n        self.ln3 = nn.LayerNorm(2000)\n        \n        self.upblock1 = nn.Sequential(nn.Linear(226, 2000),nn.GELU(),nn.BatchNorm1d(2000))\n        self.upblock2 = nn.Sequential(nn.Linear(2000,4000),nn.GELU(),nn.BatchNorm1d(4000))\n        self.upblock3 = nn.Sequential(nn.Linear(4000,7000),nn.GELU(),nn.BatchNorm1d(7000))\n        self.upblock4 = nn.Sequential(nn.Linear(7000,10000),nn.GELU(),nn.BatchNorm1d(10000))\n\n        self.downblock1 = nn.Sequential(nn.Linear(10000, 7000),nn.GELU(),nn.BatchNorm1d(7000))\n        self.downblock2 = nn.Sequential(nn.Linear(7000, 4000),nn.GELU(),nn.BatchNorm1d(4000))\n        self.downblock3 = nn.Sequential(nn.Linear(4000, 2000),nn.GELU(),nn.BatchNorm1d(2000))\n        self.downblock4 = nn.Sequential(nn.Linear(2000, 300),nn.GELU(),nn.BatchNorm1d(300))\n        \n        self.fclayer = nn.Sequential(nn.Linear(300,4))\n        \n    def forward(self, x):\n        upblock1_out = self.upblock1(x)\n        upblock2_out = self.upblock2(upblock1_out)\n        upblock3_out = self.upblock3(upblock2_out)\n        upblock4_out = self.upblock4(upblock3_out)\n        \n        downblock1_out = self.downblock1(self.ln(upblock4_out))\n        skipblock1 = downblock1_out + upblock3_out\n        downblock2_out = self.downblock2(self.ln1(skipblock1))\n        skipblock2 = downblock2_out + upblock2_out\n        downblock3_out = self.downblock3(self.ln2(skipblock2))\n        skipblock3 = downblock3_out + upblock1_out\n        downblock4_out = self.downblock4(self.ln3(skipblock3))\n        \n        output = self.fclayer(downblock4_out)\n        \n        return output","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=ResNet()\nsummary(model,(1,226))","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [1, 2000]                 --\n|    └─Linear: 2-1                       [1, 2000]                 454,000\n|    └─GELU: 2-2                         [1, 2000]                 --\n|    └─BatchNorm1d: 2-3                  [1, 2000]                 4,000\n├─Sequential: 1-2                        [1, 4000]                 --\n|    └─Linear: 2-4                       [1, 4000]                 8,004,000\n|    └─GELU: 2-5                         [1, 4000]                 --\n|    └─BatchNorm1d: 2-6                  [1, 4000]                 8,000\n├─Sequential: 1-3                        [1, 7000]                 --\n|    └─Linear: 2-7                       [1, 7000]                 28,007,000\n|    └─GELU: 2-8                         [1, 7000]                 --\n|    └─BatchNorm1d: 2-9                  [1, 7000]                 14,000\n├─Sequential: 1-4                        [1, 10000]                --\n|    └─Linear: 2-10                      [1, 10000]                70,010,000\n|    └─GELU: 2-11                        [1, 10000]                --\n|    └─BatchNorm1d: 2-12                 [1, 10000]                20,000\n├─LayerNorm: 1-5                         [1, 10000]                20,000\n├─Sequential: 1-6                        [1, 7000]                 --\n|    └─Linear: 2-13                      [1, 7000]                 70,007,000\n|    └─GELU: 2-14                        [1, 7000]                 --\n|    └─BatchNorm1d: 2-15                 [1, 7000]                 14,000\n├─LayerNorm: 1-7                         [1, 7000]                 14,000\n├─Sequential: 1-8                        [1, 4000]                 --\n|    └─Linear: 2-16                      [1, 4000]                 28,004,000\n|    └─GELU: 2-17                        [1, 4000]                 --\n|    └─BatchNorm1d: 2-18                 [1, 4000]                 8,000\n├─LayerNorm: 1-9                         [1, 4000]                 8,000\n├─Sequential: 1-10                       [1, 2000]                 --\n|    └─Linear: 2-19                      [1, 2000]                 8,002,000\n|    └─GELU: 2-20                        [1, 2000]                 --\n|    └─BatchNorm1d: 2-21                 [1, 2000]                 4,000\n├─LayerNorm: 1-11                        [1, 2000]                 4,000\n├─Sequential: 1-12                       [1, 300]                  --\n|    └─Linear: 2-22                      [1, 300]                  600,300\n|    └─GELU: 2-23                        [1, 300]                  --\n|    └─BatchNorm1d: 2-24                 [1, 300]                  600\n├─Sequential: 1-13                       [1, 4]                    --\n|    └─Linear: 2-25                      [1, 4]                    1,204\n==========================================================================================\nTotal params: 213,208,104\nTrainable params: 213,208,104\nNon-trainable params: 0\nTotal mult-adds (M): 426.20\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.76\nParams size (MB): 852.83\nEstimated Total Size (MB): 853.60\n=========================================================================================="},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}