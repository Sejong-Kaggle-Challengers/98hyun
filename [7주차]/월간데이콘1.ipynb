{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# https://dacon.io/competitions/official/235554/codeshare/634?page=1&dtype=recent&ptype=pub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport random\nimport numpy as np  # 1.18.1\nfrom numpy.random import shuffle\nimport pandas as pd  # 0.25.3\nimport torch  # 1.4.0\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data.sampler import Sampler, SequentialSampler\nfrom torch.backends import cudnn","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pytorch.org/docs/stable/notes/randomness.html\n# deterministic 결정론적 알고리즘 - 특정 입력이 들어오면 학습했던 그대로.\n# benchmark - cudnn이 결정론적 알고리즘을 선택하게 만들고. 성능을 유지시키는(REPRODUCIBILITY) 경향.\n# 아래 코드는 seed 부터 REPRODUCIBILITY 까지. 가능하게 하기 위한 코드.\n\n# https://hoya012.github.io/blog/reproducible_pytorch/\n\ntorch.manual_seed(71)\ntorch.cuda.manual_seed(71)\ntorch.cuda.manual_seed_all(71) # if use multi-GPU\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(71)\nrandom.seed(71)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pytorch.org/docs/stable/data.html\n# __len__과 __iter__를 선언해야한다. class에선 __init__ 은 당연.\n\n# https://subinium.github.io/pytorch-dataloader/ \nclass ContinuousBatchSampler(Sampler):\n    # https://hulk89.github.io/pytorch/2019/09/30/pytorch_dataset/\n    # sampler를 상속받은 새로운 Batchsampler를 만든다. index_list를 반환한다.\n    def __init__(self, sampler, batch_size, drop_last):\n        # sequentialsampler 사용한다.0,1,2,3..810000 순서로 나온다.\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last # false \n        self.from_last_epoch = []\n\n    def __iter__(self):\n        # 같은 행이 걸리면 제외하고 drop_last가 false라 버리지 않고 뒤에 합쳐진다.\n        idx_from_sampler = set(self.sampler)  # 0,1,2,3,4 810000... 에서\n#         print(f\"idx_from_sampler : {idx_from_sampler}\")\n        idx_to_exclude = set(self.from_last_epoch) # 배제할 idx\n#         print(f\"idx_to_exclude : {idx_to_exclude}\")\n        idx_after_exclusion = sorted(list(idx_from_sampler - idx_to_exclude)) \n#         print(f\"idx_after_exclusion : {idx_after_exclusion}\")\n        shuffle(idx_after_exclusion) \n        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n#         print(f\"first_batch : {first_batch}\")\n        yield first_batch\n    \n        # 이부분은 이해가 안되는게 다시 붙이고 shuffle한다. 그러면 했던게 또 걸릴텐데.\n        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n#         print(f\"idx_of_left : {idx_of_left}\")\n        shuffle(idx_of_left)\n        batch = []\n        for idx in idx_of_left:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n#                 print(f\"batch : {batch}\")\n                yield batch\n                batch = []\n        # drop_last false로 뒀기 때문에 from_last_epoch에 이전 batch 를 계속 저장.\n        if not self.drop_last:\n            self.from_last_epoch = batch.copy()\n#             print(f\"from_last_epoch : {from_last_epoch}\")\n#         print(f\"return : {(len(self.sampler) + len(self.from_last_epoch)) // self.batch_size}\")\n#         print(f\"sampler lenght : {len(self.sampler)}\")\n#         print(f\"from_last_epoch length : {len(self.from_last_epoch)}\")\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n#             print(f\"return : {(len(self.sampler) + len(self.from_last_epoch)) // self.batch_size}\")\n#             print(f\"sampler lenght : {len(self.sampler)}\")\n#             print(f\"from_last_epoch length : {len(self.from_last_epoch)}\")\n            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n\n\n### 결론\n\n1. 처음 한것은 batch가 끝나고 뒤에 붙인다 -> drop_last=False 이므로\n2. yield 때문인지는 몰라도 first_batch 이후 batch가 나오게 되면서, 처음 len(self.sampler) // self.batch_size 이후,\n (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size 게 된다. \n-> 이부분은 설명을 잘 못하겠습니다. 느낌입니다. print 끼고 하려니까 안나와서요. 훈련과정이 눈에 안보입니다.\n결국, dynamic batch 느낌입니다. 보통 batch는 2048 2048 순서적이라면\n이거 읽어보려구요. https://discuss.pytorch.org/t/dataloader-for-variable-batch-size/13840","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0') # 혹시 이 뒤에 숫자가 의미하는게 무엇인지 알고 있는분 계신가요?\nnum_epochs = 1000\nbatch_size = 2048\ninitial_learning_rate = 0.2\n# https://pytorch.org/docs/stable/data.html\n# which enables fast data transfer to CUDA-enabled GPUs.\nloader_params = {'num_workers': 8, 'pin_memory': True}\n\ntrain_data = np.array(pd.read_csv('../input/month1/data_mdc01/train.csv'), dtype=np.float32)\nX_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\ny_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n\ndataset = TensorDataset(X_train, y_train)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n\nprediction = np.zeros((10000, 4), dtype=np.float32)\nlogging_term = 1000\nlogging_total = int(810000 * num_epochs / batch_size)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://wegonnamakeit.tistory.com/47\n# batch에 있는 데이터가 변화하더라도 normalize하는 mean과 varianece 값이 바뀌지 않게 된다.\n\n# 이게 중요한것 같다. 데이터가 변해도 mean과 variance 가 그대로면 정해진 범위안에서 노는것이다. \n\nfor model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05',\n                 'Model_06', 'Model_07', 'Model_08', 'Model_09', 'Model_10',\n                 'Model_11', 'Model_12', 'Model_13', 'Model_14', 'Model_15',\n                 'Model_16', 'Model_17', 'Model_18', 'Model_19', 'Model_20']:\n\n    net = nn.Sequential(\n        nn.BatchNorm1d(226),\n        nn.ReLU(),\n        nn.Linear(226, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 4)\n    )\n\n    model = net.to(device)\n    running_loss = 0.\n    running_counter = 0\n    criterion = torch.nn.L1Loss()\n    # momentum 은 운동 관성. 관성을 넣어 학습속도를 제어한다고 한다.\n    # 학습 2에서는 weight_decay가 있는데 l2 loss라고 한다.\n    # https://deepapple.tistory.com/6 - loss를 더해서 모델의 복잡도를 낮춘다.\n    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9)\n    \n    # https://sanghyu.tistory.com/113\n    # parameter에서 cosine annealing 함수를 따르고, restart는 250개의 iter가끝나고 상황보고\n    # T_mult restart 후에 T_i를 증가시키는 factor 라는데 뜻은 모르겠다.\n    # eta_min은 최소 lr ,last epoch 는 -1이 default다. \n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n                                                               last_epoch=-1)\n    count_a=0\n    for epoch in range(num_epochs):\n\n        model.train()\n        count_b=0\n        for xx, yy in train_loader:\n            count_b+=1\n            # gpu 계산으로.\n            xx, yy = xx.to(device), yy.to(device)\n            # gradient 계산을 불가능하게 하는것.\n            with torch.no_grad():\n                # noise를 더한다.\n                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.003\n\n            out = model(xx)\n            loss = criterion(out, yy)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_counter += 1\n            running_loss += loss.item()\n            \n            # 한 epoch 당 1000번 학습\n            if running_counter % logging_term == 0:\n                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n                                                                     running_loss / logging_term))\n                running_loss = 0.\n            if count_b==2:\n                break\n        # epoch 당 한번 scheduler로 learning_rate 조절.\n        # 원래 epoch 끝나고 scheduler로 learning_rate 조절.\n        scheduler.step()\n        # 이렇게 한 이유는 epoch에 따라 어떻게 다른지 확인. 위에서 continuesampler 만들때\n        # yield가 두번 들어가는데 yield는 몇번 들어가도 상관없고, first batch 보낸 이후에 idx_left_batch 계속 보낸다.\n        if count_a==2:\n            break\n    break\n    model.eval()\n    output = model(\n        torch.tensor(np.array(pd.read_csv('../input/month1/data_mdc01/test.csv'), dtype=np.float32))[:, 1:].to(device))\n    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n    # 학습2,3,4에서는 1/7 모델을 20개 해서 0.05, 7개중의 1개라 1/7 앙상블을 뜻한다.\n    prediction += output * 0.05\n    \n    # 모델 하나당 시간이 엄청 걸린다. ","execution_count":5,"outputs":[{"output_type":"stream","text":"len(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\n","name":"stdout"},{"output_type":"stream","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n    send_bytes(obj)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\n","name":"stderr"},{"output_type":"stream","text":"len(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\nlen(first_batch) : 2048\n","name":"stdout"},{"output_type":"stream","text":"Exception in thread Thread-162:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n    c = SocketClient(address)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 620, in SocketClient\n    s.connect(address)\nFileNotFoundError: [Errno 2] No such file or directory\n\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f25b8ef11540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcount_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mcount_b\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# gpu 계산으로.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport time\nfrom itertools import chain\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n# 모델 학습을 위해 CUDA 환경 설정. : 지피유 설정\ndevice = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 별도의 데이터 Pre-Processing 과정은 없고 모델 훈련시 검증을 위해 train 중 10000개를 validation 용으로 분리.\n# 새로 만든 train.csv는 train1.csv, validation은 val.csv로 저장.\n# dataframe.sample(frac=1) 을 통해 셔플.\nfrom IPython.display import display\n\npath_train = '../input/month1/data_mdc01/train.csv'\npath_test = '../input/month1/data_mdc01/test.csv'\nlayers = [['layer_1','layer_2','layer_3','layer_4'], [str(i) for i in np.arange(0,226).tolist()]]\nlayers = list(chain(*layers))\ndisplay(len(layers))\n\ntrain = pd.read_csv(path_train)\nprint(train.shape)\ntrain = train.sample(frac=1)\nrows, cols = train.shape\n\ntrain1 = train.iloc[:rows - 10000,:]\ntrain1 = train1.values\ntrain1 = pd.DataFrame(data=train1,columns=layers)\ndisplay(train1)\n\n# train1.to_csv('train1.csv', index_label='id')\n\nprint(\"train file saved....\")\nval = train.iloc[rows - 10000:,:]\nval = val.values\nval = pd.DataFrame(data=val,columns=layers)\n# val.to_csv('val.csv', index_label='id')\ndisplay(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 새로 만든 train/ val 모델 학습 데이터 경로를 설정.\n# train_path = 'train1.csv'\n# val_path = 'val.csv'\n\nlr = 1e-03\nadam_epsilon = 1e-06\nepochs = 100\nbatch_size = 2048\nwarmup_step = 2000\nloss_fn = nn.L1Loss()\n\n# 이번에는 Dataset을 상속받은 custom dataset\nclass PandasDataset(Dataset):\n    # df로 바꿈.\n    def __init__(self, df):\n        super(PandasDataset, self).__init__()\n        train = df.iloc[:,1:]\n        self.train_X, self.train_Y = train.iloc[:,4:], train.iloc[:,0:4]\n        self.tmp_x , self.tmp_y = self.train_X.values, self.train_Y.values\n    \n    def __len__(self):\n        return len(self.train_X)\n\n    def __getitem__(self, idx):\n        return {\n            'X':torch.from_numpy(self.tmp_x)[idx],\n            'Y':torch.from_numpy(self.tmp_y)[idx]\n        }\n            \ntrain_dataset = PandasDataset(train1)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,  num_workers=4)\n\nval_dataset = PandasDataset(val)\nval_loader = DataLoader(val_dataset, batch_size=batch_size,  num_workers=4) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sigmoid = $1\\over{1+e^{-z}}$   \ntanh = $2 sigmoid(z) -1$  \ngelu = $1\\over2$ $x$ ($\\sqrt{1+{2\\over\\pi}}$) ($ x + 0.044715 * x^3 $ )  \n\n\nhttps://arxiv.org/abs/1606.08415   \nhttps://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c  \n\nGaussian Error Linear Unit (GELUs) 라 불리고, relu나 elu에 비해 잘 작동한다고 나와있다.  \n보통 computer vision 에 많이 쓰이는것같다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# activation\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \n# layer 정규화\nclass LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-5):\n        \"\"\"\n        Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.bias = nn.Parameter(torch.zeros(hidden_size))\n        self.variance_epsilon = eps\n\n        self.init_weights()\n\n    def init_weights(self):\n        self.weight.data.fill_(1.0)\n        self.bias.data.zero_()\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n        return self.weight * x + self.bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nclass skipConnectionModel(nn.Module):\n    def __init__(self):\n        super(skipConnectionModel, self).__init__()\n        \n        self.ln = LayerNorm(10000)\n        self.ln1 = LayerNorm(7000)\n        self.ln2 = LayerNorm(4000)\n        self.ln3 = LayerNorm(2000)\n        \n        self.upblock1 = nn.Sequential(nn.Linear(226, 2000),GELU(),nn.BatchNorm1d(2000))\n        self.upblock2 = nn.Sequential(nn.Linear(2000,4000),GELU(),nn.BatchNorm1d(4000))\n        self.upblock3 = nn.Sequential(nn.Linear(4000,7000), GELU(),nn.BatchNorm1d(7000))\n        self.upblock4 = nn.Sequential(nn.Linear(7000,10000),GELU(),nn.BatchNorm1d(10000))\n\n        self.downblock1 = nn.Sequential(nn.Linear(10000, 7000),GELU(),nn.BatchNorm1d(7000))\n        self.downblock2 = nn.Sequential(nn.Linear(7000, 4000),GELU(),nn.BatchNorm1d(4000))\n        self.downblock3 = nn.Sequential(nn.Linear(4000, 2000),GELU(),nn.BatchNorm1d(2000))\n        self.downblock4 = nn.Sequential(nn.Linear(2000, 300),GELU(),nn.BatchNorm1d(300))\n        \n        self.fclayer = nn.Sequential(nn.Linear(300,4))\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        upblock1_out = self.upblock1(x)\n        upblock2_out = self.upblock2(upblock1_out)\n        upblock3_out = self.upblock3(upblock2_out)\n        upblock4_out = self.upblock4(upblock3_out)\n        \n        downblock1_out = self.downblock1(self.ln(upblock4_out))\n        skipblock1 = downblock1_out + upblock3_out\n        downblock2_out = self.downblock2(self.ln1(skipblock1))\n        skipblock2 = downblock2_out + upblock2_out\n        downblock3_out = self.downblock3(self.ln2(skipblock2))\n        skipblock3 = downblock3_out + upblock1_out\n        downblock4_out = self.downblock4(self.ln3(skipblock3))\n        \n        output = self.fclayer(downblock4_out)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_constant_schedule(optimizer, last_epoch=-1):\n    \"\"\" Create a schedule with a constant learning rate.\n    \"\"\"\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n\ndef get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n    \"\"\" Create a schedule with a constant learning rate preceded by a warmup\n    period during which the learning rate increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n    \"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n    \"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer, num_warmup_steps, num_training_steps, num_cycles=1.0, last_epoch=-1\n):\n    \"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function with several hard restarts, after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdamW(Optimizer):\n    \"\"\" Implements Adam algorithm with weight decay fix.\n    Parameters:\n        lr (float): learning rate. Default 1e-3.\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n        eps (float): Adams epsilon. Default: 1e-6\n        weight_decay (float): Weight decay. Default: 0.0\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {} - should be &gt;= 0.0\".format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {} - should be &gt;= 0.0\".format(eps))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n\n                step_size = group[\"lr\"]\n                if group[\"correct_bias\"]:  # No bias correction for Bert\n                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn't interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                # Add weight decay at the end (fixed version)\n                if group[\"weight_decay\"] > 0.0:\n                    p.data.add_(-group[\"lr\"] * group[\"weight_decay\"], p.data)\n\n        return loss   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = skipConnectionModel()\nmodel = model.to(device) # 모델을 GPU 메모리에 올림.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n모델 학습\n\"\"\"\n\ntotal_step = len(train_loader) * epochs\nprint(f\"Total step is....{total_step}\") # 모델이 학습하는 전체 step 계산.\n\n# 옵티마이저와 스케줄러의 파라미터들을 정의.\n\nno_decay = [\"bias\", \"LayerNorm.weight\"] # decay하지 않을 영역 지정.\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_step, num_training_steps=total_step\n)\n\n# train loss와 val loss 지정.\ntotal_loss = 0.0\ntotal_val_loss = 0.0\n\n# 모델 이름을 위해서 변수 만듦.\nversion = time.localtime()[3:5]\ncurr_lr = lr\n\nn_val_loss = 10000000. # 가장 낮은 validation loss를 저장하기 위해서 변수 설정.\n\nfor epoch in range(epochs):\n    total_loss = 0 \n    total_val_loss = 0\n    for i, data in enumerate(tqdm(train_loader, desc='*********Train mode*******')):  # train 데이터를 부르고 학습.\n        # forward pass\n        pred = model(data['X'].float().to(device))\n        loss = loss_fn(pred, data['Y'].float().to(device))\n        \n        # backward pass\n        optimizer.zero_grad() # optimizer 객체 사용해서 학습 가능한 가중치 변수에 대한 모든 변화도를 0으로 만듦\n        loss.backward() \n        optimizer.step() # update optimizer params\n        scheduler.step() # update scheduler params\n        \n        total_loss += loss.item()\n        \n    train_loss = total_loss / len(train_loader)\n    print (\"Epoch [{}/{}], Train Loss: {:.4f}\".format(epoch+1, epochs, train_loss))\n\n    # evaluation\n    # validation 데이터를 부르고 epoch 마다 학습된 모델을 부르고 평가.\n    model.eval()\n    with torch.no_grad():\n        for i, data in enumerate(tqdm(val_loader, desc='*********Evaluation mode*******')):\n            pred = model(data['X'].float().to(device))\n            loss_val = loss_fn(pred, data['Y'].float().to(device))\n            \n            total_val_loss += loss_val.item()\n    val_loss = total_val_loss / len(val_loader)\n    print (\"Epoch [{}/{}], Eval Loss: {:.4f}\".format(epoch+1, epochs, val_loss))\n    \n    # best model을 저장.\n    if val_loss &lt; n_val_loss:\n        n_val_loss = val_loss\n        torch.save(model.state_dict(), f'test_{version}_{lr}_{epochs}.pth')\n        print(\"Best Model saved......\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n모델 테스트\n\"\"\"\n\ntest_model = skipConnectionModel()\n\n# test 파일 경로 및 test 데이터 로드\npath_test = 'test.csv'\nclass TestDataset(Dataset):\n    def __init__(self, path_test):\n        super(TestDataset, self).__init__()\n        test = pd.read_csv(path_test)\n        self.test_X = test.iloc[:,1:]\n        self.tmp_x = self.test_X.values\n    \n    def __len__(self):\n        return len(self.test_X)\n\n    def __getitem__(self, idx):\n        return torch.from_numpy(self.tmp_x)[idx]\n    \ntest_data = TestDataset(path_test)\ntest_loader = DataLoader(test_data, batch_size=10000,  num_workers=4)\n\n# 모델에 학습된 가중치를 업로드.\nweights = torch.load(f'test_{version}_{lr}_{epochs}.pth', map_location='cuda:0')\ntest_model.load_state_dict(weights)\ntest_model = test_model.to(device)\ntest_model.eval()\n\nwith torch.no_grad():\n    for data in test_loader:\n        data = data.to(device)\n        outputs = test_model(data.float())\npred_test = outputs\n\nsample_sub = pd.read_csv('sample_submission.csv', index_col=0)\nlayers = ['layer_1','layer_2','layer_3','layer_4']\nsubmission = sample_sub.values + pred_test.cpu().numpy()\n\nsubmission = pd.DataFrame(data=submission,columns=layers)\nsubmission.to_csv(f'test_{version}_{lr}_{epochs}.csv', index_label='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# + Self evaluation and Ensemble\n# Dacon의 제출하기를 통해 측정한 mae값 중 가장 낮은 mae csv 파일과 다른 파라미터 적용으로 훈련한 모델과 mae 비교를 통해\n# 대략적인 test mae를 예상한 뒤 제출하기 하여 3번 제출할 수 있는 기회를 최대한 살림.\n# 다양한 파라미터 적용을 통한 모델들을 아래의 en함수를 통해 합친 뒤 평균을 구하여 제출\n# (추가하는 csv 파일의 수에 따라 en함수의 함수가 받는 csv 파일 개수 증가 및 코드 수정 필요.)\ndef mae(best_path, my_path):\n    best = pd.read_csv(best_path)\n    best_value = best.iloc[:,1:].values\n    value = pd.read_csv(my_path)\n    my_value = value.iloc[:,1:].values\n    abs_value = abs(best_value - my_value)\n    size = abs_value.shape\n    return sum(sum(abs_value) / (size[0]*size[1]))\n\ndef en(best_path, my_path):\n    best = pd.read_csv(best_path)\n    best_value = best.iloc[:,1:].values\n    value = pd.read_csv(my_path)\n    my_value = value.iloc[:,1:].values\n    return (my_value + best_value)/2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}