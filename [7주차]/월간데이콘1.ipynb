{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# 2등 코드 리뷰\n# sampler와 ensemble을 이용.\n# 저번에 잘못 말했는데 810000/2048은 나머지가 있다. coutinue_batch_sampler는 남기는 것 없이\n# 하기위해서 drop_last를 False로 두고, 계속 이어서 하는것이다. \n# 2048/2048/.../46 이렇게 남더라. 그러면 46+2002/2048/2048/.../1072 이렇게 이어진다.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# https://dacon.io/competitions/official/235554/codeshare/634?page=1&dtype=recent&ptype=pub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /proc/cpuinfo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport random\nimport numpy as np  # 1.18.1\nfrom numpy.random import shuffle\nimport pandas as pd  # 0.25.3\nimport torch  # 1.4.0\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data.sampler import Sampler, SequentialSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pytorch.org/docs/stable/notes/randomness.html\n# deterministic 결정론적 알고리즘 - 특정 입력이 들어오면 학습했던 그대로.\n# benchmark - cudnn이 결정론적 알고리즘을 선택하게 만들고. 성능을 유지시키는(REPRODUCIBILITY) 경향.\n# 아래 코드는 seed 부터 REPRODUCIBILITY 까지. 가능하게 하기 위한 코드.\n\n# https://hoya012.github.io/blog/reproducible_pytorch/\n\ndef set_seed(seed,mode=None):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n    np.random.seed(seed)\n    random.seed(seed)\n    if mode=='reproductibility':\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \nset_seed(71,mode='reproductibility')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom_batch_sampler\n# 1. sampler,batch_size,drop_last 가 필요하다. \n# 2. __init__,__iter__,__len__ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"epoch : 1000\nbatch_size : 2048\ndatasize : 810000\n\ndatasize / batch_size = 395.5078125 항상 남는게 있다.  \n\n남는것을 batch로 해서 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://pytorch.org/docs/stable/data.html\n# __len__과 __iter__를 선언해야한다. class에선 __init__ 은 당연.\n\n# https://subinium.github.io/pytorch-dataloader/ \nclass ContinuousBatchSampler(Sampler):\n    # https://hulk89.github.io/pytorch/2019/09/30/pytorch_dataset/\n    # sampler를 상속받은 새로운 Batchsampler를 만든다. index_list를 반환한다.\n    def __init__(self, sampler, batch_size, drop_last):\n        # sequentialsampler 사용한다.0,1,2,3..810000 순서로 나온다.\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last # false \n        self.from_last_epoch = []\n\n    def __iter__(self):\n        # 같은 행이 걸리면 제외하고 drop_last가 false라 버리지 않고 뒤에 합쳐진다.\n        idx_from_sampler = set(self.sampler)  # 0,1,2,3,4 810000... 에서\n#         print(f\"len(self.sampler) : {len(self.sampler)}\")\n#         print(f\"len(self.from_last_epoch) : {len(self.from_last_epoch)}\")\n#         print(f\"idx_from_sampler : {idx_from_sampler}\")\n        idx_to_exclude = set(self.from_last_epoch) # 배제할 idx\n#         print(f\"idx_to_exclude : {idx_to_exclude}\")\n        idx_after_exclusion = list(idx_from_sampler - idx_to_exclude)\n        print(idx_after_exclusion[-5:])\n#         print(f\"idx_after_exclusion : {idx_after_exclusion}\")\n        shuffle(idx_after_exclusion) \n        first_batch = self.from_last_epoch + idx_after_exclusion[:self.batch_size - len(self.from_last_epoch)]\n#         print(f\"first_batch : {first_batch}\")\n        yield first_batch\n    \n        # 이부분은 이해가 안되는게 다시 붙이고 shuffle한다. 그러면 했던게 또 걸릴텐데.\n        idx_of_left = sorted(idx_after_exclusion[self.batch_size - len(self.from_last_epoch):] + list(idx_to_exclude))\n#         print(f\"idx_of_left : {idx_of_left}\")\n        shuffle(idx_of_left)\n        batch = []\n        for idx in idx_of_left:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n#                 print(f\"batch : {batch}\")\n                yield batch\n                batch = []\n        # drop_last false로 뒀기 때문에 from_last_epoch에 이전 batch 를 계속 저장.\n        if not self.drop_last:\n            self.from_last_epoch = batch.copy()\n#             print(f\"from_last_epoch : {from_last_epoch}\")\n#         print(f\"return : {(len(self.sampler) + len(self.from_last_epoch)) // self.batch_size}\")\n#         print(f\"sampler lenght : {len(self.sampler)}\")\n#         print(f\"from_last_epoch length : {len(self.from_last_epoch)}\")\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n#             print(f\"return : {(len(self.sampler) + len(self.from_last_epoch)) // self.batch_size}\")\n#             print(f\"sampler lenght : {len(self.sampler)}\")\n#             print(f\"from_last_epoch length : {len(self.from_last_epoch)}\")\n            return (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size\n\n\n### 결론\n\n# 1. 처음 한것은 batch가 끝나고 뒤에 붙인다 -> drop_last=False 이므로\n# 2. yield 때문인지는 몰라도 first_batch 이후 batch가 나오게 되면서, 처음 len(self.sampler) // self.batch_size 이후,\n#  (len(self.sampler) + len(self.from_last_epoch)) // self.batch_size 게 된다. \n# -> 이부분은 설명을 잘 못하겠습니다. 느낌입니다. print 끼고 하려니까 안나와서요. 훈련과정이 눈에 안보입니다.\n# 결국, dynamic batch 느낌입니다. 보통 batch는 2048 2048 순서적이라면\n# 이거 읽어보려구요. https://discuss.pytorch.org/t/dataloader-for-variable-batch-size/13840","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0') # 혹시 이 뒤에 숫자가 의미하는게 무엇인지 알고 있는분 계신가요?\nnum_epochs = 1000\nbatch_size = 2048\ninitial_learning_rate = 0.2\n# https://pytorch.org/docs/stable/data.html\n# which enables fast data transfer to CUDA-enabled GPUs.\nloader_params = {'num_workers': 0, 'pin_memory': True}\n\ntrain_data = np.array(pd.read_csv('../input/month1/data_mdc01/train.csv'), dtype=np.float32)\nX_train = torch.tensor(train_data[:, 4:], dtype=torch.float32)\ny_train = torch.tensor(train_data[:, :4], dtype=torch.float32)\n\n# dataset은 그냥 tensor 형으로 해주는것 같고.\ndataset = TensorDataset(X_train, y_train)\n# dataloaders를 정의해야한다. dataset, batch_sampler,loader_params\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ContinuousBatchSampler(\n    sampler=SequentialSampler(range(len(dataset))), batch_size=batch_size, drop_last=False), **loader_params)\n\nprediction = np.zeros((10000, 4), dtype=np.float32)\nlogging_term = 1000\nlogging_total = int(810000 * num_epochs / batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = nn.Sequential(\n        nn.BatchNorm1d(226),\n        nn.ReLU(),\n        nn.Linear(226, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 4)\n    )\n\nmodel = net.to(device)\nrunning_loss = 0.\nrunning_counter = 0\ncriterion = torch.nn.L1Loss()\n# momentum 은 운동 관성. 관성을 넣어 학습속도를 제어한다고 한다.\n# 학습 2에서는 weight_decay가 있는데 l2 loss라고 한다.\n# https://deepapple.tistory.com/6 - loss를 더해서 모델의 복잡도를 낮춘다.\noptimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9)\n\n# https://sanghyu.tistory.com/113\n# parameter에서 cosine annealing 함수를 따르고, restart는 250개의 iter가끝나고 상황보고\n# T_mult restart 후에 T_i를 증가시키는 factor 라는데 뜻은 모르겠다.\n# eta_min은 최소 lr ,last epoch 는 -1이 default다. \nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n                                                           last_epoch=-1)\n# 1 epoch 당 395번 돌리고, \nfor epoch in range(num_epochs):\n    # set mode \n    model.train()\n    for xx, yy in train_loader:\n        \n        # gpu 계산으로.\n        xx, yy = xx.to(device), yy.to(device)\n        # gradient 계산을 불가능하게 하는것.\n        with torch.no_grad():\n            # noise를 더한다.\n            xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.003\n\n        out = model(xx)\n        loss = criterion(out, yy)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_counter += 1\n        running_loss += loss.item()\n\n        if running_counter % logging_term == 0:\n            print(f'epoch {epoch} (iter {running_counter:6d}/{logging_total:6d})')\n            running_loss = 0.\n    # epoch 당 한번 scheduler로 learning_rate 조절.\n    # 원래 epoch 끝나고 scheduler로 learning_rate 조절.\n    scheduler.step()\n    # 이렇게 한 이유는 epoch에 따라 어떻게 다른지 확인. 위에서 continuesampler 만들때\n    # yield가 두번 들어가는데 yield는 몇번 들어가도 상관없고, first batch 보낸 이후에 idx_left_batch 계속 보낸다.\nmodel.eval()\noutput = model(\n    torch.tensor(np.array(pd.read_csv('../input/month1/data_mdc01/test.csv'), dtype=np.float32))[:, 1:].to(device))\noutput = np.array(output.detach().to('cpu'), dtype=np.float32)\n# 학습2,3,4에서는 1/7 모델을 20개 해서 0.05, 7개중의 1개라 1/7 앙상블을 뜻한다.\n# prediction += output ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class net(nn.Module):\n    def __init__(self):\n        # class 변수가져오기\n        super.__init__()\n#         https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html\n        self.avg=nn.AvgPool1d(kernel_size=3,stride=3,padding=1)\n        self.linear=nn.Sequential(\n            nn.Linear(76, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Linear(768, 4)\n        )\n        def forward(self,x):\n            # N? channel input\n            # 1,1,226 -> 1,1,76 -> 1,76 -> 1,768 ... -> 1,4\n            x=x.view(x.shape[0],1,-1)\n            x=self.avg(x)\n            x=x.view(x.shape[0],-1)\n            x=self.linear(x)\n            return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://wegonnamakeit.tistory.com/47\n# batch에 있는 데이터가 변화하더라도 normalize하는 mean과 varianece 값이 바뀌지 않게 된다.\n\n# 이게 중요한것 같다. 데이터가 변해도 mean과 variance 가 그대로면 정해진 범위안에서 노는것이다. \n\nfor model_no in ['Model_01', 'Model_02', 'Model_03', 'Model_04', 'Model_05',\n                 'Model_06', 'Model_07', 'Model_08', 'Model_09', 'Model_10',\n                 'Model_11', 'Model_12', 'Model_13', 'Model_14', 'Model_15',\n                 'Model_16', 'Model_17', 'Model_18', 'Model_19', 'Model_20']:\n\n    net = nn.Sequential(\n        nn.BatchNorm1d(226),\n        nn.ReLU(),\n        nn.Linear(226, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 768),\n        nn.BatchNorm1d(768),\n        nn.ReLU(),\n        nn.Linear(768, 4)\n    )\n\n    model = net.to(device)\n    running_loss = 0.\n    running_counter = 0\n    criterion = torch.nn.L1Loss()\n    # momentum 은 운동 관성. 관성을 넣어 학습속도를 제어한다고 한다.\n    # 학습 2에서는 weight_decay가 있는데 l2 loss라고 한다.\n    # https://deepapple.tistory.com/6 - loss를 더해서 모델의 복잡도를 낮춘다.\n    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9)\n    \n    # https://sanghyu.tistory.com/113\n    # parameter에서 cosine annealing 함수를 따르고, restart는 250개의 iter가끝나고 상황보고\n    # T_mult restart 후에 T_i를 증가시키는 factor 라는데 뜻은 모르겠다.\n    # eta_min은 최소 lr ,last epoch 는 -1이 default다. \n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=250, T_mult=1, eta_min=0.005,\n                                                               last_epoch=-1)\n    count_a=0\n    for epoch in range(num_epochs):\n\n        model.train()\n        count_b=0\n        for xx, yy in train_loader:\n            count_b+=1\n            # gpu 계산으로.\n            xx, yy = xx.to(device), yy.to(device)\n            # gradient 계산을 불가능하게 하는것.\n            with torch.no_grad():\n                # noise를 더한다.\n                xx += torch.randn((xx.shape[0], 226), device='cuda:0') * 0.003\n\n            out = model(xx)\n            loss = criterion(out, yy)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_counter += 1\n            running_loss += loss.item()\n            \n            # 한 epoch 당 1000번 학습\n            if running_counter % logging_term == 0:\n                print(model_no + ' (iter {:6d}/{:6d}) {:.4f}'.format(running_counter, logging_total,\n                                                                     running_loss / logging_term))\n                running_loss = 0.\n            if count_b==2:\n                break\n        # epoch 당 한번 scheduler로 learning_rate 조절.\n        # 원래 epoch 끝나고 scheduler로 learning_rate 조절.\n        scheduler.step()\n        # 이렇게 한 이유는 epoch에 따라 어떻게 다른지 확인. 위에서 continuesampler 만들때\n        # yield가 두번 들어가는데 yield는 몇번 들어가도 상관없고, first batch 보낸 이후에 idx_left_batch 계속 보낸다.\n        if count_a==2:\n            break\n    break\n    model.eval()\n    output = model(\n        torch.tensor(np.array(pd.read_csv('../input/month1/data_mdc01/test.csv'), dtype=np.float32))[:, 1:].to(device))\n    output = np.array(output.detach().to('cpu'), dtype=np.float32)\n    # 학습2,3,4에서는 1/7 모델을 20개 해서 0.05, 7개중의 1개라 1/7 앙상블을 뜻한다.\n    prediction += output * 0.05\n    \n    # 모델 하나당 시간이 엄청 걸린다. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1등 코드리뷰\n# scheduler 로 learning_rate에서 loss가 줄면 조금씩 변형하는 scheduler 사용.\n# with torch.no_grad(): 로 validation때 gradient 계산을 안하는것으로. 이렇게 해야하는것같다.\n# <> 형태의 복잡한 mlp 형태. skip-connection이 뭐 안하고 뛰어넘는다가 아니라, 뛰어넘어서 예전거도 기억하게 하는것같다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport time\nfrom itertools import chain\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n# 모델 학습을 위해 CUDA 환경 설정. : 지피유 설정\ndevice = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 별도의 데이터 Pre-Processing 과정은 없고 모델 훈련시 검증을 위해 train 중 10000개를 validation 용으로 분리.\n# 새로 만든 train.csv는 train1.csv, validation은 val.csv로 저장.\n# dataframe.sample(frac=1) 을 통해 셔플.\nfrom IPython.display import display\n\npath_train = '../input/month1/data_mdc01/train.csv'\npath_test = '../input/month1/data_mdc01/test.csv'\nlayers = [['layer_1','layer_2','layer_3','layer_4'], [str(i) for i in np.arange(0,226).tolist()]]\nlayers = list(chain(*layers))\n# display(len(layers))\n\ntrain = pd.read_csv(path_train)\n# print(train.shape)\ntrain = train.sample(frac=1)\nrows, cols = train.shape\n\ntrain1 = train.iloc[:rows - 10000,:]\ntrain1 = train1.values\ntrain1 = pd.DataFrame(data=train1,columns=layers)\n# display(train1)\n\ntrain1.to_csv('train1.csv', index_label='id')\n\nprint(\"train file saved....\")\nval = train.iloc[rows - 10000:,:]\nval = val.values\nval = pd.DataFrame(data=val,columns=layers)\nval.to_csv('val.csv', index_label='id')\n# display(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 새로 만든 train/ val 모델 학습 데이터 경로를 설정.\ntrain_path = './train1.csv'\nval_path = './val.csv'\n\nlr = 1e-03\nadam_epsilon = 1e-06\nepochs = 100\nbatch_size = 2048\nwarmup_step = 2000\nloss_fn = nn.L1Loss()\n\nclass PandasDataset(Dataset):\n    def __init__(self, path):\n        super(PandasDataset, self).__init__()\n        train = pd.read_csv(path).iloc[:,1:]\n        self.train_X, self.train_Y = train.iloc[:,4:], train.iloc[:,0:4]\n        self.tmp_x , self.tmp_y = self.train_X.values, self.train_Y.values\n    \n    def __len__(self):\n        return len(self.train_X)\n\n    def __getitem__(self, idx):\n        return {\n            'X':torch.from_numpy(self.tmp_x)[idx],\n            'Y':torch.from_numpy(self.tmp_y)[idx]\n        }\n            \ntrain_dataset = PandasDataset(train_path)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,  num_workers=4)\n\nval_dataset = PandasDataset(val_path)\nval_loader = DataLoader(val_dataset, batch_size=batch_size,  num_workers=4) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sigmoid = $1\\over{1+e^{-z}}$   \ntanh = $2 sigmoid(z) -1$  \ngelu = $1\\over2$ $x$ ($\\sqrt{1+{2\\over\\pi}}$) ($ x + 0.044715 * x^3 $ )  \n\n\nhttps://arxiv.org/abs/1606.08415   \nhttps://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c  \n\nGaussian Error Linear Unit (GELUs) 라 불리고, relu나 elu에 비해 잘 작동한다고 나와있다.  \n보통 computer vision 에 많이 쓰이는것같다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# activation\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \n# layer 정규화\nclass LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-5):\n        \"\"\"\n        Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.bias = nn.Parameter(torch.zeros(hidden_size))\n        self.variance_epsilon = eps\n\n        self.init_weights()\n\n    def init_weights(self):\n        self.weight.data.fill_(1.0)\n        self.bias.data.zero_()\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n        return self.weight * x + self.bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nclass skipConnectionModel(nn.Module):\n    def __init__(self):\n        super(skipConnectionModel, self).__init__()\n        \n        self.ln = LayerNorm(10000)\n        self.ln1 = LayerNorm(7000)\n        self.ln2 = LayerNorm(4000)\n        self.ln3 = LayerNorm(2000)\n        \n        self.upblock1 = nn.Sequential(nn.Linear(226, 2000),GELU(),nn.BatchNorm1d(2000))\n        self.upblock2 = nn.Sequential(nn.Linear(2000,4000),GELU(),nn.BatchNorm1d(4000))\n        self.upblock3 = nn.Sequential(nn.Linear(4000,7000), GELU(),nn.BatchNorm1d(7000))\n        self.upblock4 = nn.Sequential(nn.Linear(7000,10000),GELU(),nn.BatchNorm1d(10000))\n\n        self.downblock1 = nn.Sequential(nn.Linear(10000, 7000),GELU(),nn.BatchNorm1d(7000))\n        self.downblock2 = nn.Sequential(nn.Linear(7000, 4000),GELU(),nn.BatchNorm1d(4000))\n        self.downblock3 = nn.Sequential(nn.Linear(4000, 2000),GELU(),nn.BatchNorm1d(2000))\n        self.downblock4 = nn.Sequential(nn.Linear(2000, 300),GELU(),nn.BatchNorm1d(300))\n        \n        self.fclayer = nn.Sequential(nn.Linear(300,4))\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        upblock1_out = self.upblock1(x)\n        upblock2_out = self.upblock2(upblock1_out)\n        upblock3_out = self.upblock3(upblock2_out)\n        upblock4_out = self.upblock4(upblock3_out)\n        \n        downblock1_out = self.downblock1(self.ln(upblock4_out))\n        skipblock1 = downblock1_out + upblock3_out\n        downblock2_out = self.downblock2(self.ln1(skipblock1))\n        skipblock2 = downblock2_out + upblock2_out\n        downblock3_out = self.downblock3(self.ln2(skipblock2))\n        skipblock3 = downblock3_out + upblock1_out\n        downblock4_out = self.downblock4(self.ln3(skipblock3))\n        \n        output = self.fclayer(downblock4_out)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_constant_schedule(optimizer, last_epoch=-1):\n    \"\"\" Create a schedule with a constant learning rate.\n    \"\"\"\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n\ndef get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\n    \"\"\" Create a schedule with a constant learning rate preceded by a warmup\n    period during which the learning rate increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n    \"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n    \"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer, num_warmup_steps, num_training_steps, num_cycles=1.0, last_epoch=-1\n):\n    \"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function with several hard restarts, after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        # 2000으로 시작해서 current_step이 2000을 넘으면\n        # 810000//2048=395\n        # num_training_step = len(train_loader) * epochs = 보통 388 * 30 > 10000\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        # progress= 20xx-2048/10000 ? \n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        # \n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"max (0, $1 \\over 2$ ($1+ \\cos (\\pi * {progress})$))"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdamW(Optimizer):\n    \"\"\" Implements Adam algorithm with weight decay fix.\n    Parameters:\n        lr (float): learning rate. Default 1e-3.\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n        eps (float): Adams epsilon. Default: 1e-6\n        weight_decay (float): Weight decay. Default: 0.0\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {} - should be &gt;= 0.0\".format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {} - should be &gt;= 0.0\".format(eps))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n\n                step_size = group[\"lr\"]\n                if group[\"correct_bias\"]:  # No bias correction for Bert\n                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn't interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                # Add weight decay at the end (fixed version)\n                if group[\"weight_decay\"] > 0.0:\n                    p.data.add_(-group[\"lr\"] * group[\"weight_decay\"], p.data)\n\n        return loss   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = skipConnectionModel()\nmodel = model.to(device) # 모델을 GPU 메모리에 올림.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n모델 학습\n\"\"\"\n\ntotal_step = len(train_loader) * epochs\nprint(f\"Total step is....{total_step}\") # 모델이 학습하는 전체 step 계산.\n\n# 옵티마이저와 스케줄러의 파라미터들을 정의.\n\nno_decay = [\"bias\", \"LayerNorm.weight\"] # decay하지 않을 영역 지정.\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_step, num_training_steps=total_step\n)\n\n# train loss와 val loss 지정.\ntotal_loss = 0.0\ntotal_val_loss = 0.0\n\n# 모델 이름을 위해서 변수 만듦.\nversion = time.localtime()[3:5]\ncurr_lr = lr\n\nn_val_loss = 10000000. # 가장 낮은 validation loss를 저장하기 위해서 변수 설정.\n\nfor epoch in range(epochs):\n    total_loss = 0 \n    total_val_loss = 0\n    for i, data in enumerate(tqdm(train_loader, desc='*********Train mode*******')):  # train 데이터를 부르고 학습.\n        # forward pass\n        pred = model(data['X'].float().to(device))\n        loss = loss_fn(pred, data['Y'].float().to(device))\n        \n        # backward pass\n        optimizer.zero_grad() # optimizer 객체 사용해서 학습 가능한 가중치 변수에 대한 모든 변화도를 0으로 만듦\n        loss.backward() \n        optimizer.step() # update optimizer params \n        # 순서 중요.\n        scheduler.step() # update scheduler params\n        \n        total_loss += loss.item()\n        \n    train_loss = total_loss / len(train_loader)\n    print (\"Epoch [{}/{}], Train Loss: {:.4f}\".format(epoch+1, epochs, train_loss))\n\n    # evaluation\n    # validation 데이터를 부르고 epoch 마다 학습된 모델을 부르고 평가.\n    model.eval()\n    with torch.no_grad():\n        for i, data in enumerate(tqdm(val_loader, desc='*********Evaluation mode*******')):\n            pred = model(data['X'].float().to(device))\n            loss_val = loss_fn(pred, data['Y'].float().to(device))\n            \n            total_val_loss += loss_val.item()\n    val_loss = total_val_loss / len(val_loader)\n    print (\"Epoch [{}/{}], Eval Loss: {:.4f}\".format(epoch+1, epochs, val_loss))\n    \n    # best model을 저장.\n    if val_loss < n_val_loss:\n        n_val_loss = val_loss\n        torch.save(model.state_dict(), f'test_{version}_{lr}_{epochs}.pth')\n        print(\"Best Model saved......\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n모델 테스트\n\"\"\"\n\ntest_model = skipConnectionModel()\n\n# test 파일 경로 및 test 데이터 로드\npath_test = 'test.csv'\nclass TestDataset(Dataset):\n    def __init__(self, path_test):\n        super(TestDataset, self).__init__()\n        test = pd.read_csv(path_test)\n        self.test_X = test.iloc[:,1:]\n        self.tmp_x = self.test_X.values\n    \n    def __len__(self):\n        return len(self.test_X)\n\n    def __getitem__(self, idx):\n        return torch.from_numpy(self.tmp_x)[idx]\n    \ntest_data = TestDataset(path_test)\ntest_loader = DataLoader(test_data, batch_size=10000,  num_workers=4)\n\n# 모델에 학습된 가중치를 업로드.\nweights = torch.load(f'test_{version}_{lr}_{epochs}.pth', map_location='cuda:0')\ntest_model.load_state_dict(weights)\ntest_model = test_model.to(device)\ntest_model.eval()\n\nwith torch.no_grad():\n    for data in test_loader:\n        data = data.to(device)\n        outputs = test_model(data.float())\npred_test = outputs\n\nsample_sub = pd.read_csv('sample_submission.csv', index_col=0)\nlayers = ['layer_1','layer_2','layer_3','layer_4']\nsubmission = sample_sub.values + pred_test.cpu().numpy()\n\nsubmission = pd.DataFrame(data=submission,columns=layers)\nsubmission.to_csv(f'test_{version}_{lr}_{epochs}.csv', index_label='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# + Self evaluation and Ensemble\n# Dacon의 제출하기를 통해 측정한 mae값 중 가장 낮은 mae csv 파일과 다른 파라미터 적용으로 훈련한 모델과 mae 비교를 통해\n# 대략적인 test mae를 예상한 뒤 제출하기 하여 3번 제출할 수 있는 기회를 최대한 살림.\n# 다양한 파라미터 적용을 통한 모델들을 아래의 en함수를 통해 합친 뒤 평균을 구하여 제출\n# (추가하는 csv 파일의 수에 따라 en함수의 함수가 받는 csv 파일 개수 증가 및 코드 수정 필요.)\ndef mae(best_path, my_path):\n    best = pd.read_csv(best_path)\n    best_value = best.iloc[:,1:].values\n    value = pd.read_csv(my_path)\n    my_value = value.iloc[:,1:].values\n    abs_value = abs(best_value - my_value)\n    size = abs_value.shape\n    return sum(sum(abs_value) / (size[0]*size[1]))\n\ndef en(best_path, my_path):\n    best = pd.read_csv(best_path)\n    best_value = best.iloc[:,1:].values\n    value = pd.read_csv(my_path)\n    my_value = value.iloc[:,1:].values\n    return (my_value + best_value)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport math\nimport random\nfrom time import time\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, cuda\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torch.optim import Adam, SGD, Optimizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed,mode=None):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n    np.random.seed(seed)\n    random.seed(seed)\n    if mode=='reproductibility':\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \nset_seed(71)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Semi_dataset(Dataset):\n    # custom dataset은 __init__,__len__,__getitem__ 가 필요.\n    # https://wikidocs.net/57165\n    # tensor 형으로 바꿔야 한다.\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n        self.X_dataset = []\n        self.Y_dataset = []\n        for x in X:\n            # tensor형으로 바꾸고 \n            self.X_dataset.append(torch.FloatTensor(x))\n        try:\n            for y in Y.values:\n                self.Y_dataset.append(torch.tensor(y))\n        except:\n            print(\"no label\")\n            \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        # tensor 형을 내보내고 index에 따라서\n        data = self.X_dataset[index]\n        try:\n            target = self.Y_dataset[index]\n            return data, target\n        except:\n            return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataloader(X, Y, batch_size, shuffle=False):\n    # custom dataloader\n    dataset = Semi_dataset(X, Y)\n    dataloader = DataLoader(\n                            # custom dataset\n                            dataset,\n                            batch_size=batch_size,\n                            shuffle=shuffle,\n                            # cpu 갯수 잘 확인할것. 이걸로 error 많이 남. 0으로 두는데 그럼 느리다.\n                            num_workers=4\n                            )\n    return dataloader\n\n# 왜 이렇게 돌아가는지 모르겠다.\n# 평균절대오차를 구현했다.\nfrom sklearn.metrics import mean_absolute_error as mae\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \n    output_errors = np.average(np.abs(y_pred - y_true),\n                               weights=sample_weight, axis=0)\n    if isinstance(multioutput, str):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\nfrom IPython.display import display\n\na=np.array([1,2,3,4,5])\nb=np.array([1.5,2.5,2.5,4.5,6.5])\n\ndisplay(mean_absolute_error(a,b))\ndisplay(mae(a,b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdamW(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n        super(AdamW, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                if group['weight_decay'] != 0:\n                    p.data.add_(-group['weight_decay'], p.data)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP_only_flatfeatures(nn.Module):\n    def __init__(self, num_classes=1):\n        super(MLP_only_flatfeatures, self).__init__()\n        self.num_classes = num_classes         \n        self.fc_layers = nn.Sequential(\n            nn.Linear(226, 1000),\n            nn.ReLU(),\n            nn.BatchNorm1d(1000),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.BatchNorm1d(500),\n            nn.Dropout(0.1),\n            nn.Linear(500, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.1),\n            nn.Linear(128, self.num_classes)\n            )             \n        self._initialize_weights()\n\n    def forward(self, x):\n        out = self.fc_layers(x)\n        return out\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(device, model_name='mlp', weight_path=None):\n\n    if model_name == 'mlp':\n        model = MLP_only_flatfeatures(4)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation(model, criterion, valid_loader, device):\n    \n    model.eval()\n    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n    val_loss = 0.\n    \n    with torch.no_grad():\n        for i, (data, target) in enumerate(valid_loader):\n            \n            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n\n            data = data.to(device)\n            target = target.float().to(device)\n                \n            output = model(data)\n            loss = criterion(output, target)\n            \n            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n            \n            val_loss += loss.item() / len(valid_loader)\n    \n    val_score = mean_absolute_error(valid_preds, valid_targets)\n\n    return val_loss, val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if cuda.is_available:\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_PATH = '../input/month1/data_mdc01'\ntrain_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\nsubmission_df = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:, 4:], train_df.iloc[:, :4], test_size=0.02, random_state=71, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reset_index(drop=True)\nX_val = X_val.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 2048\ntrain_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\nvalid_loader = build_dataloader(X_val, y_val, batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[:, 1:] = scaler.transform(test_df.iloc[:, 1:])\n# output path\noutput_dir = Path('./', 'output')\noutput_dir.mkdir(exist_ok=True, parents=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 50\ncriterion = nn.L1Loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch_list = []\nbest_valid_score_list = []\n\n# build model\nmodel = build_model(device, model_name='mlp')\nmodel.to(device)\n\nlr = 0.001\noptimizer = AdamW(model.parameters(), lr)\n\nstart_time = time()\n\nbest_epoch = 0\nbest_train_loss = 1000\nbest_valid_score = 1000\n\nfor epoch in range(num_epochs):\n\n    model.train()\n    optimizer.zero_grad()\n    train_loss = 0.0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        if device:\n            data = data.to(device)\n            target = target.float().to(device)\n        else:\n            target = target.float()\n\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss += loss.item() / len(train_loader)\n\n    val_loss, val_score = validation(model, criterion, valid_loader, device)\n\n    elapsed = time() - start_time\n\n    lr = [_['lr'] forz _ in optimizer.param_groups]\n\n    if epoch % 2 == 0:\n        print('Epoch {} / {}  train Loss: {:.4f}  val_loss: {:.4f}  val_score: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n              .format(epoch,  num_epochs - 1, train_loss, val_loss, val_score, lr[0], elapsed // 60, elapsed % 60))\n        \n    model_path = output_dir / 'best_model.pt'\n\n    if val_score < best_valid_score:\n        best_valid_score = val_score\n        best_epoch = epoch\n        # best model params\n        torch.save(model.state_dict(), model_path)\n\n\n    best_epoch_list.append(best_epoch)\n    best_valid_score_list.append(best_valid_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch, score in zip(best_epoch_list, best_valid_score_list):\n    print(epoch, score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(best_valid_score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 2048\ntest_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(device, model_name='mlp')\nmodel.to(device)\n\nmodel.eval()\n# best params setting\nmodel.load_state_dict(torch.load(model_path))\n\ntest_preds = np.zeros((len(test_loader.dataset), 4))\n\nwith torch.no_grad():\n    for batch_idx, data in enumerate(test_loader):\n        if device:\n            data = data.to(device)\n        outputs = model(data)\n        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': submission_df['id'],\n                           'layer_1':test_preds.transpose()[0],\n                           'layer_2':test_preds.transpose()[1],\n                           'layer_3':test_preds.transpose()[2],\n                           'layer_4':test_preds.transpose()[3]})\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ntrain = pd.read_csv('../input/month1/data_mdc01/train.csv')\ntest = pd.read_csv('../input/month1/data_mdc01/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#독립변수와 종속변수를 분리합니다.\ntrain_X = train.iloc[:,4:]\ntrain_Y = train.iloc[:,:4]\ntest_X = test.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(units=160, activation='relu', input_dim=226))\nmodel.add(Dense(units=160, activation='relu'))\nmodel.add(Dense(units=160, activation='relu'))\nmodel.add(Dense(units=4, activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#모델을 컴파일합니다.\nmodel.compile(loss='mae', optimizer='adam', metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#모델을 학습합니다.\nmodel.fit(train_X, train_Y, epochs=20, batch_size=10000, validation_split = 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#예측값을 생성합니다.\npred_test = model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission 파일을 생성합니다.\nsample_sub = pd.read_csv('../input/month1/data_mdc01/sample_submission.csv', index_col=0)\nsubmission = sample_sub+pred_test\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular.all import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=Path('../input/month1/data_mdc01')\n\ntrain=pd.read_csv(path/'train.csv')\ntest=pd.read_csv(path/'test.csv')\nsubmit=pd.read_csv(path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_names=[str(i) for i in range(226)]\n\ny_names=['layer_1','layer_2','layer_3','layer_4']\nprocs=[Normalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=train.drop(y_names,axis=1)\n# prepare\ntest=test.drop('id',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls=TabularDataLoaders.from_df(train,path=path,cont_names=cont_names,\n                               procs=procs,y_names=y_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn=tabular_learner(dls,lr=0.003)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.validate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dl=learn.dls.test_dl(test)\npred,_=learn.get_preds(dl=test_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}