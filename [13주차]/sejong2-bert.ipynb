{"cells":[{"metadata":{},"cell_type":"markdown","source":"## BERT - Encoder stack of transformer architecture. \n[개념](https://www.kaggle.com/ritesh2000/bert-all-in-one)  \n[코드](https://www.kaggle.com/dionysios1981/bert-pytorch-tweets)"},{"metadata":{},"cell_type":"markdown","source":"bert를 사용하는 이유\n\n1. 데이터의 부족. -> 이번 데이터는 너무 많아서 줄였다. \n2. 적은 데이터로도 큰 결과.  \n3. 전이학습. -> pre-trained 돼서 훨씬 사용이 간편하다고 한다. \n\n\nbert의 t는 transformer이고 이 변환은 기존의 rnn의 방식인 한칸이 아니라 서로 상호작용을 할 수 있게 했다. \n\n방법 \n\n전처리 - tokenizer \ninput으로 바뀐후 세개로 나눠 들어간다.\n\ntoken embedding. segment embedding, positional embedding. \n\n특징은 mask 인데 \n\nmask가 뭐냐. 쉽게 말하면 가리고 자리 맞추기 훈련해보는.  \n\n나중에 padding 해서 글자길이 맞춰주는데 padding으로 인한 token은 0이다. 의미없어으니까 mask를 씌워서 mask가 된곳만 훈련을 하는 그런식으로. 그리고 코드를 보며 설명.","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sejong-ai-challenge-p2/train.csv',usecols=[1,2])\ntest= pd.read_csv('/kaggle/input/sejong-ai-challenge-p2/test.csv',usecols=[1])\nsample_submission=pd.read_csv('/kaggle/input/sejong-ai-challenge-p2/sample_submission.csv')","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 시간 너무 오래걸려서 \ntrain=train.sample(frac=0.1)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()\n\n# nan 값 확인.\n# train=train.dropna()","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"Text     0\nLabel    0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, texts, targets, tokenizer, max_len):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        # tokenizer는 str로 받는다. 혹은 list of str. \n        text = self.texts[item]\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            # cls와 sep등 token을 넣어준다. bert 규칙.\n            add_special_tokens=True,\n            # 그런데 max_length를 정해줬으니까 절단을 시켜줘야한다. \n            max_length=self.max_len,\n            return_token_type_ids=False,\n            # truncation 이 절단이데 절단 안시키면 512로 그대로 들어간다.\n            truncation=True,\n            # 그리고 이건 바뀐부분 같은데 pad_to_max_length가 예전에 truncation과 max_length를 같이 했다면\n            # padding 자체는 이런기능이 없어서 나눠줘야 하는것 같다. \n            padding='max_length',\n            # return attetion_mask는 주목할 mask 를 알려주는것.\n            return_attention_mask=True,\n            # tensor를 알려준다. pt 는 pytorch tensor. tf는 tensorflow numpy는 np\n            return_tensors='pt'\n        )\n        \n        return {\n          'text': text,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERT(nn.Module):\n    \n    def __init__(self):\n        super(BERT, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')       \n        self.fc = nn.Linear(768,2)\n        \n    def forward(self, input_ids, attention_mask):\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        output = output[\"pooler_output\"]\n        return self.fc(output)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train,df_test=train_test_split(train,test_size=0.2,random_state=71)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = TrainDataset(texts=df.Text.to_numpy(),targets=df.Label.to_numpy(),\n                      tokenizer=tokenizer,max_len=max_len)    \n    return DataLoader(ds,batch_size=batch_size)\n\nMAX_LEN=64\n# 빠르게 간만 보려고 512했는데 oom, 256도 oom \nBATCH_SIZE = 128\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=BERT().to(device)\nEPOCHS = 3\noptimizer = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\nloss_fn = nn.CrossEntropyLoss().to(device)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_loss = np.inf\nfor epoch in range(3):\n    model.train()\n    for d in train_data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        loss.backward()\n        # 그래디언트 폭주 막기 위해 쓴다고 한다.\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    model = model.eval()\n    valid_loss = 0\n    with torch.no_grad():\n        for d in val_data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n             )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            valid_loss += loss.item()\n            valid_loss /= len(val_data_loader)\n    print(f\"EPOCH:{epoch}, Loss:{valid_loss}\")\n    if valid_loss < best_loss:\n        best_loss = valid_loss\n        torch.save(model.state_dict(), \"nlpmodel.pth\")\n        print(\"saved...\")","execution_count":53,"outputs":[{"output_type":"stream","text":"EPOCH:0, Loss:0.003396518323073142\nsaved...\nEPOCH:1, Loss:0.002583795433797997\nsaved...\nEPOCH:2, Loss:0.0019358544131108048\nsaved...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = self.texts[item]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n         )\n        return {\n          'text': text,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten()\n          }","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = TestDataset(texts=test.Text.to_numpy(),tokenizer=tokenizer,max_len=max_len)\n    return DataLoader(ds,batch_size=batch_size)\n\nMAX_LEN=64\nBATCH_SIZE = 64\ntest_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\nmodel.load_state_dict(torch.load(\"./nlpmodel.pth\"))","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_preds = []\n\nmodel.eval()\nwith torch.no_grad():\n    for d in test_data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n            \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n             )\n        _, preds = torch.max(outputs, dim=1)\n        submit_preds.extend(preds)\n    predictions = torch.stack(submit_preds).cpu()\n","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=predictions.cpu().detach().numpy()\nsample_submission[\"Label\"] = pred\nprint(sample_submission.head())","execution_count":58,"outputs":[{"output_type":"stream","text":"   id  Label\n0   0      1\n1   1      0\n2   2      0\n3   3      0\n4   4      0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission.to_csv(\"submit_bert.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}